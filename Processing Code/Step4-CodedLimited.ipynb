{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad629436-6f13-493e-8f61-78f15b2c51f9",
   "metadata": {},
   "source": [
    "This file converts the Research Identified Files into Limited Dataset Files.\n",
    "\n",
    "Author: John Lawrence, Date 3/7/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aefa342-a9bb-4f3d-bcf1-903e7200efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id,rand,col,round,to_timestamp,date_sub,expr,date_format,lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bf3cb23-6729-4ddf-8fa4-8b513ece9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all the Ross datasets\n",
    "tdfar19 = spark.read.parquet('/fs/ess/scratch/PAS2164/CarescapeComb/Ross2019Alarm.parquet')\n",
    "tdfwr19 = spark.read.parquet('/fs/ess/scratch/PAS2164/CarescapeComb/Ross2019Wave.parquet')\n",
    "tdfmr19 = spark.read.parquet('/fs/ess/scratch/PAS2164/CarescapeComb/Ross2019Measurement.parquet')\n",
    "tdfar20 = spark.read.parquet('/fs/ess/scratch/PAS2164/CarescapeComb/Ross2020Alarm.parquet')\n",
    "tdfwr20 = spark.read.parquet('/fs/ess/scratch/PAS2164/CarescapeComb/Ross2020Wave.parquet')\n",
    "tdfmr20 = spark.read.parquet('/fs/ess/scratch/PAS2164/CarescapeComb/Ross2020Measurement.parquet')\n",
    "#tdfar21 = spark.read.parquet('/fs/ess/scratch/PAS2164/CarescapeComb/Ross2021Alarm.parquet')\n",
    "#tdfwr21 = spark.read.parquet('/fs/ess/scratch/PAS2164/CarescapeComb/Ross2021Wave.parquet')\n",
    "#tdfmr21 = spark.read.parquet('/fs/ess/scratch/PAS2164/CarescapeComb/Ross2021Measurement.parquet')\n",
    "\n",
    "#Load all the James datasets\n",
    "tdfaj19 = spark.read.parquet('/fs/ess/scratch/PAS2164/CarescapeComb/James2019Alarm.parquet')\n",
    "tdfwj19 = spark.read.parquet('/fs/ess/scratch/PAS2164/CarescapeComb/James2019Wave.parquet')\n",
    "tdfmj19 = spark.read.parquet('/fs/ess/scratch/PAS2164/CarescapeComb/James2019Measurement.parquet')\n",
    "tdfaj20 = spark.read.parquet('/fs/ess/scratch/PAS2164/CarescapeComb/James2020Alarm.parquet')\n",
    "tdfwj20 = spark.read.parquet('/fs/ess/scratch/PAS2164/CarescapeComb/James2020Wave.parquet')\n",
    "tdfmj20 = spark.read.parquet('/fs/ess/scratch/PAS2164/CarescapeComb/James2020Measurement.parquet')\n",
    "#tdfaj21 = spark.read.parquet('/fs/ess/scratch/PAS2164/CarescapeComb/James2021Alarm.parquet')\n",
    "#tdfwj21 = spark.read.parquet('/fs/ess/scratch/PAS2164/CarescapeComb/James2021Wave.parquet')\n",
    "#tdfmj21 = spark.read.parquet('/fs/ess/scratch/PAS2164/CarescapeComb/James2021Measurement.parquet')\n",
    "\n",
    "#If this is not the first time running th is, load the existing bed and person tables to preserve identifiers and date offset.\n",
    "dfBed = spark.read.parquet('/fs/ess/scratch/PAS2164/LocationMap.parquet')\n",
    "dfcsid = spark.read.parquet('/fs/ess/scratch/PAS2164/PatientMap.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ef743a-e1cf-40d0-a07d-ca965a3cb28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all the James datasets into a 3 master datasets\n",
    "tdfar2=tdfar19.unionByName(tdfar20)\n",
    "#tdfar2=tdfar1.unionByName(tdfar21)\n",
    "tdfwr2=tdfwr19.unionByName(tdfwr20)\n",
    "#tdfwr2=tdfwr1.unionByName(tdfwr21)\n",
    "tdfmr2=tdfmr19.unionByName(tdfmr20)\n",
    "#tdfmr2=tdfmr1.unionByName(tdfmr21)\n",
    "\n",
    "#Combine all the ross datasets into 3 master datasets\n",
    "tdfaj2=tdfaj19.unionByName(tdfaj20)\n",
    "#tdfaj2=tdfaj1.unionByName(tdfaj21)\n",
    "tdfwj2=tdfwj19.unionByName(tdfwj20)\n",
    "#tdfwj2=tdfwj1.unionByName(tdfwj21)\n",
    "tdfmj2=tdfmj19.unionByName(tdfmj20)\n",
    "#tdfmj2=tdfmj1.unionByName(tdfmj21)\n",
    "\n",
    "#Combine all the ross and james datasets into single datasets tdfa* is temp dataframe alarm j for james, r for ross.\n",
    "tempdfa=tdfar2.unionByName(tdfaj2)\n",
    "tempdfw=tdfwr2.unionByName(tdfwj2)\n",
    "tempdfm=tdfmr2.unionByName(tdfmj2)\n",
    "\n",
    "#So at this point we have 3 dataframes, one for each of the three different data tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2511823-9f69-4744-b591-270e6dd9ddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to mask beds, so I am going to make a unique list of beds from all three datasets. \n",
    "bedA=tempdfa[\"assignedLocationCareArea\",\"assignedLocationRoom\",\"assignedLocationBed\"].distinct()\n",
    "bedW=tempdfw[\"assignedLocationCareArea\",\"assignedLocationRoom\",\"assignedLocationBed\"].distinct()\n",
    "bedM=tempdfm[\"assignedLocationCareArea\",\"assignedLocationRoom\",\"assignedLocationBed\"].distinct()\n",
    "#Then union this list of beds into 1 dataset\n",
    "dfBed1=bedA.unionByName(bedW)\n",
    "dfBed2=dfBed1.unionByName(bedM)\n",
    "#Then identify the unique beds\n",
    "dfBed3=dfBed2.distinct()\n",
    "#And generate a unique ID for each bed. \n",
    "dfBed=dfBed3.withColumn(\"csBedID\", monotonically_increasing_id())\n",
    "#Note, why I planned to mask beds the beds in this list and the hospital beds didn't actually match so it ended up being moot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c27b262d-233d-46a2-b36e-ae74c9237031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I want to do the same thing I just did with beds but with patient identifiers. Identify unique patient identifiers, unify them, and then find all the distict ones.\n",
    "csidA=tempdfa['patientIdPrimary-id','patientIdPrimary-type'].distinct()\n",
    "csidW=tempdfw['patientIdPrimary-id','patientIdPrimary-type'].distinct()\n",
    "csidM=tempdfm['patientIdPrimary-id','patientIdPrimary-type'].distinct()\n",
    "dfcsid1=csidA.unionByName(csidW)\n",
    "dfcsid2=dfcsid1.unionByName(csidM)\n",
    "dfcsid3=dfcsid2.distinct()\n",
    "#In addition to a unique patient identifier I also want to build a random 14 day offset by patient ID. So I generate that here as well.\n",
    "dfcsid4=dfcsid3.withColumn(\"Offset\",round((rand()*28)-14).cast(\"Integer\"))\n",
    "dfcsid=dfcsid4.withColumn(\"csID\", monotonically_increasing_id())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f234028-9eaf-4329-84e9-de1e54f7be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next I need to map the original 3 tables to the patientID table\n",
    "tempdfa1=tempdfa.join(dfcsid,tempdfa[\"patientIdPrimary-id\"]==dfcsid[\"patientIdPrimary-id\"],\"Inner\")\n",
    "tempdfw1=tempdfw.join(dfcsid,tempdfw[\"patientIdPrimary-id\"]==dfcsid[\"patientIdPrimary-id\"],\"Inner\")\n",
    "tempdfm1=tempdfm.join(dfcsid,tempdfm[\"patientIdPrimary-id\"]==dfcsid[\"patientIdPrimary-id\"],\"Inner\")\n",
    "#And to the locationID table.\n",
    "tempdfa2=tempdfa1.join(dfBed,[tempdfa1[\"assignedLocationCareArea\"]==dfBed[\"assignedLocationCareArea\"],tempdfa1[\"assignedLocationRoom\"]==dfBed[\"assignedLocationRoom\"],tempdfa1[\"assignedLocationBed\"]==dfBed[\"assignedLocationBed\"]],\"Inner\")\n",
    "tempdfw2=tempdfw1.join(dfBed,[tempdfw1[\"assignedLocationCareArea\"]==dfBed[\"assignedLocationCareArea\"],tempdfw1[\"assignedLocationRoom\"]==dfBed[\"assignedLocationRoom\"],tempdfw1[\"assignedLocationBed\"]==dfBed[\"assignedLocationBed\"]],\"Inner\")\n",
    "tempdfm2=tempdfm1.join(dfBed,[tempdfm1[\"assignedLocationCareArea\"]==dfBed[\"assignedLocationCareArea\"],tempdfm1[\"assignedLocationRoom\"]==dfBed[\"assignedLocationRoom\"],tempdfm1[\"assignedLocationBed\"]==dfBed[\"assignedLocationBed\"]],\"Inner\")\n",
    "#And I need to remove the un-necessary dataset.\n",
    "tempdfa3=tempdfa2['csID','csBedID',\"Offset\",'polltime','alarmName','abnormalFlags','inactivationState','sil','setLow','setHigh','chanValue']\n",
    "tempdfm3=tempdfm2['csID','csBedID',\"Offset\",'polltime','mesname','msite','muom','mtext']\n",
    "tempdfw3=tempdfw2['csID','csBedID',\"Offset\",'polltime','mgname','mgGain','mgHZ','mgwave','mguom','mgsite','mgscale','mginvalid','mgmissing','mgPoints','mgPointsBytes','mgMin','mgMax','mgOffset']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a3df12-e090-42f9-bf82-e69b3086802c",
   "metadata": {
    "tags": []
   },
   "source": [
    "At this point there are 5 new datasets\n",
    "\n",
    "tempdfa3,m3,w3 which are the coded limited datasets\n",
    "\n",
    "\n",
    "dfBed and dfcsid which are the maps to the codes (one to their location, one to their identity)\n",
    "\n",
    "\n",
    "For this to become a deidentified dataset, the next step is to shift the dates in polltime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "830b5e95-1405-4d97-919a-13d177f43365",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdfa4=tempdfa3.withColumn(\"polltimestamp\",to_timestamp(\"polltime\"))\n",
    "tempdfw4=tempdfw3.withColumn(\"polltimestamp\",to_timestamp(\"polltime\"))\n",
    "tempdfm4=tempdfm3.withColumn(\"polltimestamp\",to_timestamp(\"polltime\"))\n",
    "tempdfa5=tempdfa4.withColumn(\"offsetDate\",expr(\"date_sub(polltimestamp,Offset)\"))\n",
    "tempdfw5=tempdfw4.withColumn(\"offsetDate\",expr(\"date_sub(polltimestamp,Offset)\"))\n",
    "tempdfm5=tempdfm4.withColumn(\"offsetDate\",expr(\"date_sub(polltimestamp,Offset)\"))\n",
    "tempdfa6=tempdfa5.withColumn(\"PollDate\",expr(\"date_sub(polltimestamp,0)\"))\n",
    "tempdfw6=tempdfw5.withColumn(\"PollDate\",expr(\"date_sub(polltimestamp,0)\"))\n",
    "tempdfm6=tempdfm5.withColumn(\"PollDate\",expr(\"date_sub(polltimestamp,0)\"))\n",
    "tempdfa7=tempdfa6.withColumn('offsetTime', date_format('polltimestamp', 'HH:mm:ss'))\n",
    "tempdfw7=tempdfw6.withColumn('offsetTime', date_format('polltimestamp', 'HH:mm:ss'))\n",
    "tempdfm7=tempdfm6.withColumn('offsetTime', date_format('polltimestamp', 'HH:mm:ss'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d2b22b-f3af-45a8-94ba-661e41df0147",
   "metadata": {},
   "source": [
    "At this point we have a completed coded dataset for all of the carescape data; however, I only have EMR data for a 9 month window, and the dataset is already enormous, so I filter it to only have that 9 month window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b2c4a6d-9175-4455-99c8-8f212ebe69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filterLogic=((tempdfm7['PollDate'] >= \"2019-11-23\") & (tempdfm7['PollDate'] <= \"2020-08-23\"))\n",
    "tempdfm8=tempdfm7.filter(filterLogic)\n",
    "filterLogic=((tempdfa7['PollDate'] >= \"2019-11-23\") & (tempdfa7['PollDate'] <= \"2020-08-23\"))\n",
    "tempdfa8=tempdfa7.filter(filterLogic)\n",
    "filterLogic=((tempdfw7['PollDate'] >= \"2019-11-23\") & (tempdfw7['PollDate'] <= \"2020-08-23\"))\n",
    "tempdfw8=tempdfw7.filter(filterLogic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8d235-4362-46f1-b272-bb40aebfcdb3",
   "metadata": {},
   "source": [
    "Finally, I remove the identified variables making this a coded limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0664b384-51d6-43a2-9f4b-60deea981323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tempdfa9=tempdfa8['csID','csBedID','offsetDate','offsetTime','alarmName','abnormalFlags','inactivationState','sil','setLow','setHigh','chanValue']\n",
    "tempdfw9=tempdfw8['csID','csBedID','offsetDate','offsetTime','mgname','mgGain','mgHZ','mgwave','mguom','mgsite','mgscale','mginvalid','mgmissing','mgPoints','mgPointsBytes','mgMin','mgMax','mgOffset']\n",
    "tempdfm9=tempdfm8['csID','csBedID','offsetDate','offsetTime','mesname','msite','muom','mtext']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a96a3d6-ca6a-4486-b66b-8ae24cf3419a",
   "metadata": {},
   "source": [
    "tempdfa9,w9,and m9 are the 3 final datasets that are then combined with dfBed and dfcsid to make that coded limited.\n",
    "\n",
    "All that's left now is to map these to the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2e8e1ae-6e38-434b-af74-eea77b6d245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdfa9.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").parquet('/fs/ess/scratch/PAS2164/Alarms.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1111639-5ffc-45ef-a843-98d2cf8190a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdfw9.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").parquet('/fs/ess/scratch/PAS2164/Waveforms.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86d2747a-aeba-426e-be80-c72a4c2ef5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdfm9.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").parquet('/fs/ess/scratch/PAS2164/Messages.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b28e80e7-f671-44ee-b90e-3171f3b5d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfBed.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").parquet('/fs/ess/scratch/PAS2164/LocationMap.parquet')\n",
    "dfcsid.write.option(\"mergeSchema\", \"true\").mode(\"overwrite\").parquet('/fs/ess/scratch/PAS2164/PatientMap.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c4c176-3587-4640-8549-9fd2fa585061",
   "metadata": {},
   "source": [
    "Before the release of the dataset dfBed and dfcsid will be deleted converting this dataset from a coded limited dataset to a limited data set.\n",
    "\n",
    "With the Coded limited dataset created, the next code to run is the EMR Mapping program."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
